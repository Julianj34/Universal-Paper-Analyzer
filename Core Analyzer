import os
import fitz  # PyMuPDF for PDF parsing
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor
import time

# Load environment variables
load_dotenv()

# Setup ChatGPT Model (can be adjusted based on OpenAI account settings)
MODEL_NAME = os.getenv("GPT_MODEL", "gpt-4o")
TEMPERATURE = float(os.getenv("GPT_TEMPERATURE", 0.3))
chatgpt_model = ChatOpenAI(model_name=MODEL_NAME, temperature=TEMPERATURE)

# === System Message: Defines persona and language ===
SYSTEM_PROMPT = """
You are an interdisciplinary research assistant with expertise in complexity science, systems theory, and scientific methodology.
Analyze scientific texts deeply and clearly. Use precise and structured academic English in your responses.
Highlight key relationships, systemic patterns, and innovative perspectives.
"""

# === Macro Prompt for overall synthesis ===
MACRO_PROMPT = """
Conduct a comprehensive, systemic analysis of this scientific paper based on the following five core principles:

1. Systemic Definition (Central Questions and Objectives)
2. Interdisciplinarity (Relevant Disciplines)
3. Deep Data & Networking (Key Data Sources)
4. Pattern Recognition (Core Patterns and Mechanisms)
5. Validation & Creativity (Critical Points and Innovations)

Create a structured, interdisciplinary synthesis that reveals systemic connections, identifies key patterns, and integrates innovative approaches for problem-solving.
"""

# === Micro Prompts ===
PROMPTS = {
    "Systemic Definition": """
Formulate the central research questions and long-term objectives of this paper.
What systemic relationships are to be understood? What are the key assumptions and boundaries?
""",
    "Interdisciplinarity": """
Identify the relevant scientific disciplines contributing to this research.
What interdisciplinary synergies are critical to understanding the topic deeply?
""",
    "Deep Data & Networking": """
Determine the key data sources and system connections necessary for comprehensive insight.
Which data flows and networked structures are relevant to represent the system‚Äôs complexity?
""",
    "Pattern Recognition": """
Identify the key patterns, nonlinear dynamics, and emergent mechanisms in the system.
Are there recurring structures or feedback loops?
""",
    "Validation & Creativity": """
Evaluate the strengths and weaknesses of the approach.
Which creative or alternative methodologies could improve the model or deepen insight?
"""
}


def extract_text_from_pdf(pdf_file):
    """Extracts all text from a given PDF file."""
    with fitz.open(pdf_file) as doc:
        text = "".join(page.get_text() for page in doc)
    return text


def count_tokens(text):
    """Counts the approximate number of tokens in a text."""
    return len(text.split())


def chunk_text(text, chunk_size=3500):
    """Splits the text into smaller chunks to avoid token limit errors."""
    words = text.split()
    return [" ".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]


def analyze_chunk(chunk, prompt):
    """Analyze a single chunk using the GPT model with a specific prompt."""
    while True:
        try:
            messages = [
                SystemMessage(content=SYSTEM_PROMPT),
                SystemMessage(content=prompt),
                HumanMessage(content=chunk)
            ]
            response = chatgpt_model.invoke(messages)
            return response.content.strip()
        except Exception as e:
            if "rate_limit_exceeded" in str(e).lower():
                print("Rate limit reached. Waiting 60 seconds...")
                time.sleep(60)
            else:
                print(f"Error analyzing chunk: {e}")
                return f"Error: {e}"


def analyze_paper(pdf_file, output_file):
    """Analyze the given research paper using the GPT model with both macro and micro prompts."""
    paper_text = extract_text_from_pdf(pdf_file)
    print(f"Token count: {count_tokens(paper_text)}")
    chunks = chunk_text(paper_text, chunk_size=3500)
    results = []

    # Macro analysis
    print("üîç Starting macro analysis...")
    macro_results = [analyze_chunk(chunk, MACRO_PROMPT) for chunk in chunks]
    results.append("=== Macro Analysis ===\n" + "\n\n".join(macro_results))

    # Micro analyses
    for prompt_key, prompt in PROMPTS.items():
        print(f"üß© Starting micro analysis: {prompt_key}")
        prompt_results = [analyze_chunk(chunk, prompt) for chunk in chunks]
        results.append(f"=== {prompt_key} ===\n" + "\n\n".join(prompt_results))

    with open(output_file, "w", encoding="utf-8") as f:
        f.write("\n\n".join(results))
    print(f"‚úÖ Analysis complete. Output saved to: {output_file}")


def main():
    pdf_file = input("Enter the path to your PDF file: ").strip()
    output_file = pdf_file.replace(".pdf", "_analysis.txt")

    if os.path.exists(pdf_file):
        print(f"üöÄ Starting analysis for file: {pdf_file}")
        analyze_paper(pdf_file, output_file)
    else:
        print(f"‚ùå File not found: '{pdf_file}'")


if __name__ == "__main__":
    main()

